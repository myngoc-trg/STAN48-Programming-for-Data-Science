{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92162c8e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "018c8dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import libraries done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Import libraries done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3017c111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression class defined.\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, Y, X):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def loglik(self, betas):\n",
    "        # X = (self.X - np.mean(self.X, axis=1)) / np.std(self.X, axis=1)\n",
    "        X = self.X\n",
    "        return betas @ X.T @ self.Y - np.sum(np.log(1 + np.exp(X @ betas)))\n",
    "\n",
    "    def grad_loglik(self, betas):\n",
    "        # X = (self.X - np.mean(self.X, axis=1)) / np.std(self.X, axis=1)\n",
    "        X = self.X\n",
    "        return X.T @ (self.Y - 1 / (1 + np.exp(-X @ betas)))\n",
    "\n",
    "    def hess_loglik(self, betas):\n",
    "        # X = (self.X - np.mean(self.X, axis=1)) / np.std(self.X, axis=1)\n",
    "        X = self.X\n",
    "        p = 1 / (1 + np.exp(-X @ betas))\n",
    "        W = np.diag(p * (1 - p))\n",
    "        return -X.T @ W @ X\n",
    "    \n",
    "    def newton_raphson(self, betas, tol=1e-6, max_iter=100):\n",
    "        \"\"\" Use this method to find zeros of the gradient of the log-likelihood function.\n",
    "        \n",
    "        Algorithm:\n",
    "        betas_new = betas - f(betas) / f'(betas)\n",
    "        where f is the gradient of the log-likelihood function and f' is the Hessian of the log-likelihood function.\n",
    "\n",
    "        So:\n",
    "        betas_new = betas - grad_loglik(betas) @ inv(hess_loglik(betas))\n",
    "        Use np.linalg.solve instead of np.linalg.inv for better numerical stability. And it's expensive to compute the inverse of a matrix.\n",
    "\n",
    "        Update betas until convergence or max_iter is reached.\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            # f_betas is the gradient of the log-likelihood function at betas\n",
    "            f_betas = self.grad_loglik(betas)\n",
    "\n",
    "            # f_grad_betas is the Hessian of the log-likelihood function at betas\n",
    "            f_grad_betas = self.hess_loglik(betas)\n",
    "            \n",
    "            betas_new = betas + np.linalg.solve(f_grad_betas, f_betas)\n",
    "\n",
    "            if np.linalg.norm(betas_new - betas) < tol:\n",
    "                print(f\"Converged in {i+1} iterations.\")\n",
    "                return betas_new\n",
    "\n",
    "            betas = betas_new\n",
    "        print(\"Max iterations reached.\")\n",
    "        return betas\n",
    "    def __str__(self):\n",
    "        return f\"LogisticRegression with {self.X.shape[1]} predictors and {self.X.shape[0]} observations.\"    \n",
    "\n",
    "print(\"LogisticRegression class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ff912",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
